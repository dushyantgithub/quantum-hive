# quantum-hive

# ğŸ§  Jarvis AI Assistant on Raspberry Pi

A fully functional **Jarvis-like AI assistant** built using a **Raspberry Pi 4B (8GB)**, touch screen, mic, camera, and speaker. This assistant can **listen**, **think**, **respond**, and even **control smart home devices**â€”all while displaying an interactive 3D avatar interface with animations and face tracking.

## ğŸ’¡ Project Overview

This open-source project aims to replicate a real-time, responsive AI assistant similar to J.A.R.V.I.S from Iron Man, using only low-cost, open tools and hardware.

### ğŸ”§ Core Features

- ğŸ™ï¸ **Speech-to-Text** (STT) using free, offline-capable models like [Vosk](https://alphacephei.com/vosk/)
- ğŸ§  **AI Brain** powered by Open Source LLMs or lightweight APIs
- ğŸ”Š **Text-to-Speech** (TTS) using models like Coqui TTS or Pico TTS
- ğŸ’¬ **Natural Conversations** with context awareness
- ğŸ‘ï¸ **Face Tracking** with the Pi camera (OpenCV + face landmarks)
- ğŸ“± **Smart Home Integration** via Google Home API / Matter API
- ğŸ–¥ï¸ **Electron App UI** with a 3D animated avatar (Three.js / Anime.js)
- ğŸŒ **Remote Access** by turning Pi into a secure web-accessible server

---

## ğŸ–¥ï¸ Hardware Requirements

- Raspberry Pi 4B (8GB RAM)
- 5.5" Touchscreen display
- USB Microphone
- USB Speaker or 3.5mm Audio Output
- Raspberry Pi Camera Module
- SD Card (32GB+ recommended)
- Power Supply
- Optional: External casing, cooling fan, and power bank

---

## ğŸ“¦ Software Stack

| Component         | Technology                              |
|------------------|------------------------------------------|
| OS               | Raspberry Pi OS Lite (64-bit)            |
| STT              | Vosk + Python bindings                   |
| AI Engine        | OpenAI API or Local LLM (e.g. GPT4All)   |
| TTS              | Coqui TTS / PicoTTS                      |
| Face Tracking    | OpenCV + Dlib / Mediapipe                |
| UI Frontend      | Electron + Three.js / Anime.js           |
| Backend API      | Node.js or Python Flask server           |
| Home Control     | Google Home Graph API / Matter Protocol  |
| Remote Access    | Ngrok / Tailscale / Custom SSH Tunnel    |

---

## ğŸš€ Getting Started

### ğŸ”Œ 1. Hardware Setup

1. Install Raspberry Pi OS on SD card.
2. Connect touchscreen, mic, speaker, and camera.
3. Enable camera and audio in `raspi-config`.

### ğŸ§± 2. Install Dependencies

```bash
sudo apt update && sudo apt upgrade
sudo apt install python3-pip portaudio19-dev libasound2-dev libatlas-base-dev
pip3 install vosk openai flask coqui-tts opencv-python


(Install Electron and other UI packages in the ui/ directory separately)

âš™ï¸ 3. Configure Environment
Create a .env file in your root project:

OPENAI_API_KEY=your-key
GOOGLE_HOME_API_TOKEN=your-google-token
USE_LOCAL_LLM=false
TTS_ENGINE=coqui
ğŸ§  4. Run the Assistant
python3 main.py
Or to launch the UI version:

npm install
npm start
ğŸ“¡ Remote Access

Set up Tailscale or Ngrok for secure remote access:

Tailscale guide
Ngrok
ğŸ¤– Avatar + UI

Located in the ui/ folder. Built using:

Electron
Three.js (3D avatar)
Anime.js (gesture animations)
Socket connection to Pi backend
ğŸ  Smart Home Integration

Control lights, devices, and more via:

Google Home Graph API
(Future) Matter Protocol via ESP32 + Zigbee integration
ğŸ“· Face Tracking

Real-time face tracking using:

OpenCV for face detection
Dlib or Mediapipe for landmarks
Camera controls avatar gaze
ğŸ§° Future Additions

Emotion detection via tone analysis
LLM-based long-term memory
Wake word detection ("Initiate Hive Mind")
Custom device control over MQTT or Zigbee
Whisper STT integration for better accuracy
ğŸ¤ Contributing

Pull requests are welcome! For major changes, please open an issue first to discuss.

ğŸ“„ License

This project is licensed under the MIT License.

ğŸ‘¤ Creator

Made with â¤ï¸ by Dushyant Rathore

Inspired by Tony Stark, powered by open source.

---


